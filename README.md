# Hands-on LLM Fine-Tuning & Post-Training

This repository contains my personal notes and hands-on implementations for fine-tuning and post-training Large Language Models (LLMs). The exercises are inspired by and collected from the DeepLearning.AI short course:

**“Fine-tuning & Reinforcement Learning for LLMs: Intro to Post-training”**  
https://learn.deeplearning.ai/courses/fine-tuning-and-reinforcement-learning-for-llms-intro-to-post-training

## What this repo covers

- Practical notebooks and scripts for supervised fine-tuning (SFT) of LLMs on custom datasets.
- Post-training workflows to adapt pre-trained models to specific tasks (Q&A, reasoning, chat, etc.).
- Data preparation: instruction-style formatting, tokenization, padding, and batching for causal LMs.
- Training utilities: tracking train/validation loss, playing with learning rates and other hyperparameters.
- Small, focused examples that are easy to read, run, and extend for your own experiments.[file:16]

## How to use

- Clone the repo and open the notebooks in your favorite environment (VS Code, Jupyter, Colab, etc.).
- Read the inline notes to understand each step of the fine-tuning and post-training pipeline.
- Swap in your own datasets and model checkpoints to build task-specific LLMs with minimal changes.

## Credits

- Original learning materials and ideas from DeepLearning.AI’s  
  “Fine-tuning & Reinforcement Learning for LLMs: Intro to Post-training”.[attached_file:1]
- Notes, code organization, and additional experiments by **Injamul Haque (AI/ML researcher and engineer)**.
