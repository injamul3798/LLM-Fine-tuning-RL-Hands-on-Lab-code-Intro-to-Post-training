# -*- coding: utf-8 -*-
"""contract_analysis_ai_qlora_finetuning_tinyllama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BvvZAY-RqvIvs0fdXjN3127XejDUxdCp
"""

print('its working')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LLM-training

!pip -q install -U transformers datasets accelerate peft trl bitsandbytes
print('success')

import os
os.environ["HF_TOKEN"] = "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"

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, prepare_model_for_kbit_training
from trl import SFTConfig, SFTTrainer

base = "https://huggingface.co/datasets/kiddothe2b/contract-nli/resolve/refs%2Fconvert%2Fparquet"

data_files = {
    "train":      f"{base}/contractnli_a/train/0000.parquet",
    "validation": f"{base}/contractnli_a/validation/0000.parquet",
    "test":       f"{base}/contractnli_a/test/0000.parquet",
}

ds = load_dataset("parquet", data_files=data_files)
print(ds["train"][0])
print(ds["train"][1])

# print label feature info
print(ds["train"].features["label"])

label_map = {0: "entailment", 1: "contradiction", 2: "neutral"}

def format_example(ex):
    label = label_map[int(ex["label"])]
    return {
        "text": (
            "### Instruction:\n"
            "Classify the hypothesis using the contract text as entailment, contradiction, or not_mentioned.\n\n"
            f"### Contract:\n{ex['premise']}\n\n"
            f"### Hypothesis:\n{ex['hypothesis']}\n\n"
            "### Label:\n"
            f"{label}"
        )
    }

train_ds = ds["train"].shuffle(seed=42).select(range(min(1200, len(ds["train"])))).map(
    format_example, remove_columns=ds["train"].column_names
)
eval_ds  = ds["validation"].shuffle(seed=7).select(range(min(200, len(ds["validation"])))).map(
    format_example, remove_columns=ds["validation"].column_names
)

print(train_ds[0]["text"][:700])

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

bnb = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token
tokenizer.model_max_length = 512

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    target_modules="all-linear",
    task_type="CAUSAL_LM",
)

out_dir = "contractnli-a-qlora-tinyllama"

args = SFTConfig(
    output_dir=out_dir,
    max_steps=120,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=60,
    eval_strategy="steps",
    eval_steps=60,
    fp16=False,
    bf16=False,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    peft_config=peft_config,
    processing_class=tokenizer,
)

trainer.train()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
LORA_DIR = "/content/LLM-training/contractnli-a-qlora-tinyllama/checkpoint-120"

# tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.pad_token = tokenizer.eos_token

# 4-bit config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# load base model in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto",
)

# load LoRA adapter
model = PeftModel.from_pretrained(model, LORA_DIR)
model.eval()

def classify(contract, hypothesis):
    prompt = (
        "### Instruction:\n"
        "Classify the hypothesis using the contract text as entailment, contradiction, or not_mentioned.\n\n"
        f"### Contract:\n{contract}\n\n"
        f"### Hypothesis:\n{hypothesis}\n\n"
        "### Label:\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=5,
            do_sample=False,
            temperature=0.0,
        )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text.split("### Label:")[-1].strip()

# ---- Test case ----
contract = "This Agreement shall be governed by the laws of the State of California."
hypothesis = "The contract is governed by the laws of the State of New York."


print("Prediction:", classify(contract, hypothesis))

